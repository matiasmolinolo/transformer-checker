{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dyck_k_generator import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "elif device == \"cuda:0\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dyck_k_generator.generator import generate_dataset\n",
    "\n",
    "path = generate_dataset(\n",
    "    n=1000,\n",
    "    k=k,\n",
    "    max_length=10,\n",
    "    balanced=0.55,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = \"\".join(\n",
    "    [\"\".join((key, value)) for key, value in list(constants.BRACKETS.items())[:k]]\n",
    ")\n",
    "VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset import DyckLanguageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DyckLanguageDataset(path, VOCAB).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.15 * train_size)\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hooked Transformer (Bidirectional mask) - Dyck-1 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.hooked_transformer import (\n",
    "    TransformerClassifier,\n",
    "    TransformerClassifierConfig,\n",
    "    causal_mask,\n",
    "    pad_token_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = TransformerClassifierConfig(\n",
    "    vocab_size=len(VOCAB),\n",
    "    d_model=256,\n",
    "    n_heads=1,\n",
    "    dim_ff=384,\n",
    "    n_layers=1,\n",
    "    n_classes=2,\n",
    "    max_seq_len=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bidirectional = TransformerClassifier(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bidirectional.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model_bidirectional.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, val_loss, val_acc = model_bidirectional.train_model(\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    optimizer=optimizer,\n",
    "    criterion=crit,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    use_mask=\"bidirectional\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_bidirectional.eval_model(\n",
    "    device=device,\n",
    "    test_dataloader=test_dataloader,\n",
    "    criterion=crit,\n",
    "    use_mask=\"bidirectional\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss, label=\"train loss\")\n",
    "plt.plot(val_loss, label=\"val loss\")\n",
    "plt.plot(train_acc, label=\"train acc\")\n",
    "plt.plot(val_acc, label=\"val acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hooked Transformer (causal mask) - Dyck-1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_causal = TransformerClassifier(model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model_causal.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, val_loss, val_acc = model_causal.train_model(\n",
    "    device=device,\n",
    "    epochs=50,\n",
    "    optimizer=optimizer,\n",
    "    criterion=crit,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    use_mask=\"causal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_causal.eval_model(\n",
    "    device=device,\n",
    "    test_dataloader=test_dataloader,\n",
    "    criterion=crit,\n",
    "    use_mask=\"causal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss, label=\"train loss\")\n",
    "plt.plot(val_loss, label=\"val loss\")\n",
    "plt.plot(train_acc, label=\"train acc\")\n",
    "plt.plot(val_acc, label=\"val acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention plots:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "strings, labels, tokens = batch\n",
    "\n",
    "mask = pad_token_mask(tokens)\n",
    "attn_matrices = model_bidirectional.get_attn_matrices(tokens, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_viz.visualizer import min_max_normalize, plot_attn_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset import DyckLanguageTokenizer\n",
    "from dyck_k_generator.checker import is_dyck_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (\n",
    "    \")))))))(((((((\",\n",
    "    is_dyck_word(\")))))))(((((((\", k=1),\n",
    "    DyckLanguageTokenizer(VOCAB).tokenize(\")))))))(((((((\").to(device),\n",
    ")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_matrices(VOCAB, batch, model_bidirectional, min_max_normalize, pad_token_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimax_norm = min_max_normalize(attn_matrices[0][0][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimax_norm[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hooked Transformer (Bidirectional mask) - Dyck-3 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = generate_dataset(\n",
    "    n=5_000,\n",
    "    k=3,\n",
    "    min_length=8,\n",
    "    max_length=8,\n",
    "    balanced=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = \"\".join(\n",
    "    [\"\".join((key, value)) for key, value in list(constants.BRACKETS.items())[:k]]\n",
    ")\n",
    "VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dyck_2 = DyckLanguageDataset(path, VOCAB).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset_dyck_2))\n",
    "val_size = int(0.15 * train_size)\n",
    "test_size = len(dataset_dyck_2) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset_dyck_2, [train_size, val_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = TransformerClassifierConfig(\n",
    "    vocab_size=len(VOCAB),\n",
    "    d_model=512,\n",
    "    n_heads=1,\n",
    "    dim_ff=1024,\n",
    "    n_layers=1,\n",
    "    n_classes=2,\n",
    "    max_seq_len=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerClassifier(model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, val_loss, val_acc = model.train_model(\n",
    "    device=device,\n",
    "    epochs=15,\n",
    "    optimizer=optimizer,\n",
    "    criterion=crit,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    use_mask=\"bidirectional\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.eval_model(\n",
    "    device=device,\n",
    "    test_dataloader=test_dataloader,\n",
    "    criterion=crit,\n",
    "    use_mask=\"bidirectional\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss, label=\"train loss\")\n",
    "plt.plot(val_loss, label=\"val loss\")\n",
    "plt.plot(train_acc, label=\"train acc\")\n",
    "plt.plot(val_acc, label=\"val acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_matrices(VOCAB, batch, model, min_max_normalize, pad_token_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of Distribution Dyck-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyck_3_train_dataset = generate_dataset(\n",
    "    n=50_000,\n",
    "    k=3,\n",
    "    min_length=96,\n",
    "    max_length=96,\n",
    "    balanced=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyck_3_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = \"\".join(\n",
    "    [\"\".join((key, value)) for key, value in list(constants.BRACKETS.items())[:k]]\n",
    ")\n",
    "VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyck_3_train = DyckLanguageDataset(dyck_3_train_dataset, VOCAB).to(device)\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dyck_3_train, [0.8, 0.2]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_model_config = TransformerClassifierConfig(\n",
    "    vocab_size=len(VOCAB),\n",
    "    d_model=256,\n",
    "    n_heads=1,\n",
    "    dim_ff=320,\n",
    "    n_layers=2,\n",
    "    n_classes=2,\n",
    "    max_seq_len=128,\n",
    ")\n",
    "\n",
    "ood_model = TransformerClassifier(ood_model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(ood_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, val_loss, val_acc = ood_model.train_model(\n",
    "    device=device,\n",
    "    epochs=10,\n",
    "    optimizer=optimizer,\n",
    "    criterion=crit,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    use_mask=\"bidirectional\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss, label=\"train loss\")\n",
    "plt.plot(val_loss, label=\"val loss\")\n",
    "plt.plot(train_acc, label=\"train acc\")\n",
    "plt.plot(val_acc, label=\"val acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyck_3_test_dataset = generate_dataset(\n",
    "    n=10_000,\n",
    "    k=3,\n",
    "    min_length=32,\n",
    "    max_length=128,\n",
    "    balanced=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyck_3_test = DyckLanguageDataset(dyck_3_test_dataset, VOCAB).to(device)\n",
    "test_dataloader = DataLoader(dyck_3_test, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = ood_model.eval_model(\n",
    "    device=device,\n",
    "    test_dataloader=test_dataloader,\n",
    "    criterion=crit,\n",
    "    use_mask=\"bidirectional\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-checker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
