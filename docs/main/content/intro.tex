\chapter{Introduction}

Large Language Models (LLMs) have been a topic of interest in the field of Computer Science for the past few years, 
and more recently, with the release of ChatGPT~\cite{chatgpt} by OpenAI, they have become a topic of interest for the general public too.

These models are based on an architecture called Transformer~\cite{attention_is_all_you_need}, a type of Artificial Neural Network (ANN) 
well suited to process sequences, such as text. These models have grown exponentially, as seen in~\ref{fig:model_sizes}, both in size and complexity, in the last years, and 
have shown to achieve state-of-the-art results in a wide variety of Natural Language Processing (NLP) and Natural Language Understanding (NLU) tasks.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{model_sizes}
    \caption{Model Sizes (2018--2021)~\cite{model_sizes}}
    \label{fig:model_sizes}
\end{figure}

However, despite their state-of-the-art performance, the inner workings of these models are not yet fully understood and these models are still considered opaque or \emph{black-box}~\cite{lei-etal-2016-rationalizing}, which is a problem for their adoption in critical applications, such as healthcare or finance 
where decisions need to be explainable and interpretable. 

Moreover, there is still a boundary to the capabilities of these models, regarding which problems can or cannot be solved by them and what can be learned and expressed by these models.

A discussion on the differences between learning and expressivity in these models is necessary to define our research question, as the gist of this problem is linguistics.

The objective of this thesis is to explore the attention patterns and neural activations in Transformer architectures for sequence classification, more specifically, in context-free grammars.
This work is based on the hypothesis that the attention patterns and neural activations in these models can be used to explain the decisions made by the model, and that these explanations can be used to improve the model's performance and interpretability.

\bigskip

\textbf{Outline}

Chapter 1 will introduce the concepts behind formal languages and context-free grammars, focusing especially on the Chomsky Hierarchy and Dyck-$k$ languages.
Chapter 2 will introduce the Transformer architecture, with a focus towards the attention mechanism.
Chapter 3 will discuss related works and the state-of-the-art in the field of explainable AI and formal languages
Chapter 4 focuses on the experimental setup, the dataset used, the model architecture, the training process and the obtained results.
Chapter 5 will discuss the results obtained and the implications of these results.
Chapter 6 will summarize the work done and propose future work.