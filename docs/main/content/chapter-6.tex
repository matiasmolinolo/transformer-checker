\chapter{Conclusions and Future Work}

Throughout this work, we carried out these experiments that focused on interpretability and explainability of language models, more specifically, Transformer classifiers. We discussed formal languages and the Chomsky Hierarchy, context-free languages such as Dyck-$k$ and Shuffle-Dyck-$k$, the Transformer architecture and its components, the state-of-the-art of using these models to classify sequences belonging to a formal language and the engineering process behind the development of the \verb|transformer-checker| tool. We discussed how expressive, trainable and interpretable these models are, through the lens of \emph{mechanistic interpretability}, which can be likened to seeing the execution of a program and trying to reverse-engineer it. In our case, we looked at the model's internal components (attention matrices, weights) and tried to draw conclusions on the model's decision from these components. 

We conducted a series of experiments using Transformer classifiers to determine whether sequences belong to a context-free language, such as Dyck-$k$. We found these models to be computationally sufficient for this task, reaching perfect accuracies under certain conditions. We found that sequence length does not hinder the performance of these models, as we tested sequence lengths of up to 4096. We did find, however, an important limitation of these models - their ability to classify sequences accurately is bounded by the lengths present in the training data.

Through these experiments, we discovered that the mask applied to the input sequences plays a pivotal role in the model's ability to learn effectively. The correct use of masking ensures that the model focuses on the appropriate parts of the sequence, which is critical for learning the hierarchical and nested structure characteristic of Dyck-$k$ languages. 

Additionally, we found that the self-attention mechanism within the Transformer is central to the model's decision-making process. The attention mechanism appears to capture important dependencies between tokens, allowing the model to learn which parts of the sequence relate to each other, when a pad-token mask is used. Specifically, self-attention helps the model track the relationships between opening and closing symbols, which is essential for determining membership in Dyck-$k$ languages. Thus, the information encoded in the self-attention weights provides valuable insight into how the model arrives at its classification decisions. 

We were able to visualize this by extracting and plotting the attention matrices of these models, which gave us an intuitive way of looking inside the Transformer and making sense of its output, effectively switching the black-box approach that is more commonly used with language models for a white-box approach.

Furthermore, we explored the internal workings of other components in the models, such as the weights of the feed-forward layer responsible of outputting the classification probability distribution and found no useful information or patterns that could help clarify the model's decision process.

We compared our results with those obtained by Str√∂bl et al.~\cite{strobl2024formal}, Yao et al.~\cite{bounded-hierarchical-languages} and Ebrahimi et al.~\cite{sa-nets-dyck-n} and found similarities, differences and limitations on the experiments conducted by the authors, aside from the findings described above. Firstly, we found that albeit causally-masked Transformers may be \emph{expressive} enough to classify Dyck-$k$ languages, they are not \emph{trainable} enough to do this consistently and with perfect accuracy. Moreover, we found that one-layer Transformers do not consistently achieve perfect accuracy on this task, and that a minimum of two layers are required to consistently achieve 100\% accuracy on Dyck-$k$ sequence classification.

We developed \verb|transformer-checker|, a tool that can be easily integrated into \verb|neuralchecker|, the existing explainability tool developed by the Universidad ORT Uruguay's AI research group. Our tool expands the capability of \verb|neuralchecker| by providing a way to analyze complex models, such as Transformers, with a novel, white-box approach.

Regarding future work, we look forward to combining this approach with automata extraction, similar to the work done by Weiss et al.~\cite{weiss-rnn-automata}, Carrasco et al.~\cite{carrasco-llm} and Mayr et al.~\cite{mayr-automata}, as well as working with sparse autoencoders to extract interpretable features \cite{sae-lm, bricken-monosemanticity}, 
%as proposed by Cunningham et al.~and Bricken et al.,
in order to be able to work with larger models and work on problems larger than toy examples of interpretability of language models for formal languages.

