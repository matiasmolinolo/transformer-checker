@misc{chatgpt,
  url     = {https://openai.com/index/chatgpt},
  journal = {Introducing ChatGPT},
  author  = {OpenAI},
  year    = {2022},
  month   = {Nov}
}

@misc{attention_is_all_you_need,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762}
}

@misc{model_sizes,
  title   = {Large language models: A new Moore’s Law?},
  url     = {https://huggingface.co/blog/large-language-models},
  journal = {Hugging Face – The AI community building the future.},
  author  = {Simon, Julien},
  year    = {2021},
  month   = {Oct}
}

@inproceedings{lei-etal-2016-rationalizing,
  title     = {Rationalizing Neural Predictions},
  author    = {Lei, Tao  and
               Barzilay, Regina  and
               Jaakkola, Tommi},
  editor    = {Su, Jian  and
               Duh, Kevin  and
               Carreras, Xavier},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2016},
  address   = {Austin, Texas},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D16-1011},
  doi       = {10.18653/v1/D16-1011},
  pages     = {107--117}
}

@misc{explainable-ai,
      title={Explainable AI: current status and future directions}, 
      author={Prashant Gohel and Priyanka Singh and Manoranjan Mohanty},
      year={2021},
      eprint={2107.07045},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.07045}, 
}

@misc{mech-interp-ai-safety,
      title={Mechanistic Interpretability for AI Safety -- A Review}, 
      author={Leonard Bereska and Efstratios Gavves},
      year={2024},
      eprint={2404.14082},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2404.14082}, 
}

@misc{strobl2024formal,
  title         = {What Formal Languages Can Transformers Express? A Survey},
  author        = {Lena Strobl and William Merrill and Gail Weiss and David Chiang and Dana Angluin},
  year          = {2024},
  eprint        = {2311.00208},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2311.00208}
}

@incollection{context-free-chomsky,
  title     = {The Algebraic Theory of Context-Free Languages*},
  editor    = {P. Braffort and D. Hirschberg},
  series    = {Studies in Logic and the Foundations of Mathematics},
  publisher = {Elsevier},
  volume    = {35},
  pages     = {118-161},
  year      = {1963},
  booktitle = {Computer Programming and Formal Systems},
  issn      = {0049-237X},
  doi       = {https://doi.org/10.1016/S0049-237X(08)72023-8},
  url       = {https://www.sciencedirect.com/science/article/pii/S0049237X08720238},
  author    = {N. Chomsky and M.P. Schützenberger},
  abstract  = {Publisher Summary
               This chapter discusses the several classes of sentence-generating devices that are closely related, in various ways, to the grammars of both natural languages and artificial languages of various kinds. By a language it simply mean a set of strings in some finite set V of symbols called the vocabulary of the language. By a grammar a set of rules that give a recursive enumeration of the strings belonging to the language. It can be said that the grammar generates these strings. The chapter discusses the aspect of the structural description of a sentence, namely, its subdivision into phrases belonging to various categories. A major concern of the general theory of natural languages is to define the class of possible strings; the class of possible grammars; the class of possible structural descriptions; a procedure for assigning structural descriptions to sentences, given a grammar; and to do all of this in such a way that the structural description assigned to a sentence by the grammar of a natural language will provide the basis for explaining how a speaker of this language would understand this sentence.}
}

@article{chomsky-hierarchy,
  title    = {On certain formal properties of grammars},
  journal  = {Information and Control},
  volume   = {2},
  number   = {2},
  pages    = {137-167},
  year     = {1959},
  issn     = {0019-9958},
  doi      = {https://doi.org/10.1016/S0019-9958(59)90362-6},
  url      = {https://www.sciencedirect.com/science/article/pii/S0019995859903626},
  author   = {Noam Chomsky},
  abstract = {A grammar can be regarded as a device that enumerates the sentences of a language. We study a sequence of restrictions that limit grammars first to Turing machines, then to two types of system from which a phrase structure description of the generated language can be drawn, and finally to finite state Markov sources (finite automata). These restrictions are shown to be increasingly heavy in the sense that the languages that can be generated by grammars meeting a given restriction constitute a proper subset of those that can be generated by grammars meeting the preceding restriction. Various formulations of phrase structure description are considered, and the source of their excess generative power over finite state sources is investigated in greater detail.}
}

@article{attention-tc,
  author  = {Jorge Pérez and Pablo Barceló and Javier Marinkovic},
  title   = {Attention is Turing-Complete},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {75},
  pages   = {1--35},
  url     = {http://jmlr.org/papers/v22/20-302.html}
}

@book{hopcroft-automata,
  author    = {Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D.},
  title     = {Introduction to Automata Theory,  Languages, and Computation (3rd Edition)},
  year      = {2006},
  isbn      = {0321462254},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  address   = {USA}
}

@book{leeuwen-cfg,
  editor    = {Jan van Leeuwen},
  title     = {Handbook of Theoretical Computer Science, Volume {B:} Formal Models
               and Semantics},
  publisher = {Elsevier and {MIT} Press},
  year      = {1990},
  url       = {https://www.sciencedirect.com/book/9780444880741/formal-models-and-semantics},
  isbn      = {0-444-88074-7},
  timestamp = {Tue, 06 Aug 2019 09:45:21 +0200},
  biburl    = {https://dblp.org/rec/books/el/Leeuwen90a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{rozenberg1997handbook,
  title     = {Handbook of Formal Languages: Volume 1. Word, Language, Grammar},
  author    = {Rozenberg, G. and Salomaa, A.},
  isbn      = {9783540604204},
  lccn      = {lc96047134},
  series    = {Handbook of Formal Languages},
  url       = {https://books.google.com.uy/books?id=yQ59ojndUt4C},
  year      = {1997},
  publisher = {Springer}
}

@misc{badhanau-attn,
      title={Neural Machine Translation by Jointly Learning to Align and Translate},
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473},
}

@misc{bounded-hierarchical-languages,
      title={Self-Attention Networks Can Process Bounded Hierarchical Languages},
      author={Shunyu Yao and Binghui Peng and Christos Papadimitriou and Karthik Narasimhan},
      year={2023},
      eprint={2105.11115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2105.11115},
}

@misc{bhattamistra-transformers-formal-languages,
      title={On the Ability and Limitations of Transformers to Recognize Formal Languages},
      author={Satwik Bhattamishra and Kabir Ahuja and Navin Goyal},
      year={2020},
      eprint={2009.11264v2},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.11264}
}

@misc{mech_interp, 
    title={Toy Models of Superposition}, 
    url={https://transformer-circuits.pub/2022/toy_model/index.html}, 
    author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and et al.}, 
    year={2022}, month={Sep}
} 

@misc{layernorm,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1607.06450}, 
}

@misc{pytorch-max,
	author = {{Pytorch Foundation} and {Pytorch Contributors}},
	title = {torch.max; {P}y{T}orch 2.4 documentation - pytorch.org},
	url = {https://pytorch.org/docs/stable/generated/torch.max.html},
	year = {2023},
}

@software{poetry,
author = {Eustace, Sébastien and {The Poetry contributors}},
license = {MIT},
title = {{Poetry: Python packaging and dependency management made easy}},
url = {https://github.com/python-poetry/poetry}
}

@software{typer,
author = {Ramírez, Sebastián},
license = {MIT},
title = {{Typer}},
url = {https://github.com/fastapi/typer}
}

@inproceedings{pytorch,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
booktitle = {29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)},
doi = {10.1145/3620665.3640366},
month = apr,
publisher = {ACM},
title = {{PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation}},
url = {https://pytorch.org/assets/pytorch2-2.pdf},
year = {2024}
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@misc{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@article{hahn-transformer,
    author = {Hahn, Michael},
    title = "{Theoretical Limitations of Self-Attention in Neural Sequence Models}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {156-171},
    year = {2020},
    month = {01},
    abstract = "{Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00306},
    url = {https://doi.org/10.1162/tacl\_a\_00306},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00306/1923102/tacl\_a\_00306.pdf},
}

@software{hf-tokenizer,
    author = {Moi, Anthony and Patry, Nicolas},
    license = {Apache-2.0},
    month = {April},
    title = {{HuggingFace's Tokenizers}},
    url = {https://github.com/huggingface/tokenizers},
    version = {0.13.4},
    year = {2023}
}

@Inbook{handbook-formal-languages,
author="Mateescu, Alexandru
and Salomaa, Arto",
editor="Rozenberg, Grzegorz
and Salomaa, Arto",
title="Formal Languages: an Introduction and a Synopsis",
bookTitle="Handbook of Formal Languages: Volume 1 Word, Language, Grammar",
year="1997",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--39",
abstract="What is a language? By consulting a dictionary one finds, among others, the following explanations:1The body of words and systems for their use common to people who are of the same community or nation, the same geographical area, or the same cultural tradition.2Any set or system of signs or symbols used in a more or less uniform fashion by a number of people who are thus enabled to communicate intelligibly with one other.3Any system of formalized symbols, signs, gestures, or the like, used or conceived as a means of communicating thought, emotion, etc.",
isbn="978-3-642-59136-5",
doi="10.1007/978-3-642-59136-5_1",
url="https://doi.org/10.1007/978-3-642-59136-5_1"
}

@Inbook{handbook-cfg,
author="Autebert, Jean-Michel
and Berstel, Jean
and Boasson, Luc",
editor="Rozenberg, Grzegorz
and Salomaa, Arto",
title="Context-Free Languages and Pushdown Automata",
bookTitle="Handbook of Formal Languages: Volume 1 Word, Language, Grammar",
year="1997",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="111--174",
abstract="This chapter is devoted to context-free languages. Context-free languages and grammars were designed initially to formalize grammatical properties of natural languages [9]. They subsequently appeared to be well adapted to the formal description of the syntax of programming languages. This led to a considerable development of the theory.",
isbn="978-3-642-59136-5",
doi="10.1007/978-3-642-59136-5_3",
url="https://doi.org/10.1007/978-3-642-59136-5_3"
}

@book{kleene-star,
  title     = "Foundations of discrete mathematics",
  author    = "Fletcher, Peter and Hoyle, Hughes and Patty, C Wayne",
  publisher = "Brooks/Cole",
  month     =  nov,
  year      =  1990,
  address   = "Florence, KY"
}

@misc{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@inproceedings{sa-nets-dyck-n,
    title = "How Can Self-Attention Networks Recognize {D}yck-n Languages?",
    author = "Ebrahimi, Javid  and
      Gelda, Dhruv  and
      Zhang, Wei",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.384",
    doi = "10.18653/v1/2020.findings-emnlp.384",
    pages = "4301--4306",
    abstract = "We focus on the recognition of Dyck-n (Dn) languages with self-attention (SA) networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of SA, one with a starting symbol (SA+) and one without (SA-). Our results show that SA+ is able to generalize to longer sequences and deeper dependencies. For D2, we find that SA- completely breaks down on long sequences whereas the accuracy of SA+ is 58.82{\%}. We find attention maps learned by SA+ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of SA networks is at par with LSTMs, which provides evidence on the ability of SA to learn hierarchies without recursion.",
}

@article{weiss-rnn-automata,
author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
year = {2022},
month = {06},
pages = {},
title = {Extracting automata from recurrent neural networks using queries and counterexamples (extended version)},
volume = {113},
journal = {Machine Learning},
doi = {10.1007/s10994-022-06163-2}
}

@misc{carrasco-llm,
      title={Analyzing constrained LLM through PDFA-learning}, 
      author={Matías Carrasco and Franz Mayr and Sergio Yovine and Johny Kidd and Martín Iturbide and Juan Pedro da Silva and Alejo Garat},
      year={2024},
      eprint={2406.08269},
      archivePrefix={arXiv},
      primaryClass={cs.FL},
      url={https://arxiv.org/abs/2406.08269}, 
}

@misc{mayr-automata,
      title={Towards Efficient Active Learning of PDFA}, 
      author={Franz Mayr and Sergio Yovine and Federico Pan and Nicolas Basset and Thao Dang},
      year={2022},
      eprint={2206.09004},
      archivePrefix={arXiv},
      primaryClass={cs.FL},
      url={https://arxiv.org/abs/2206.09004}, 
}

@misc{sae-lm,
      title={Sparse Autoencoders Find Highly Interpretable Features in Language Models}, 
      author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
      year={2023},
      eprint={2309.08600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.08600}, 
}

@article{bricken-monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }

